\documentclass[12pt]{article}
\usepackage{pmmeta}
\pmcanonicalname{1Introduction}
\pmcreated{2014-04-22 16:27:47}
\pmmodified{2014-04-22 16:27:47}
\pmowner{rspuzio}{6075}
\pmmodifier{rspuzio}{6075}
\pmtitle{1. Introduction}
\pmrecord{11}{88085}
\pmprivacy{1}
\pmauthor{rspuzio}{6075}
\pmtype{Feature}
\pmclassification{msc}{94A17}
\pmclassification{msc}{60J20}
\pmclassification{msc}{81P15}
\pmclassification{msc}{18F20}

% this is the default PlanetMath preamble.  as your knowledge
% of TeX increases, you will probably want to edit this, but
% it should be fine as is for beginners.

% almost certainly you want these
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% need this for including graphics (\includegraphics)
\usepackage{graphicx}
% for neatly defining theorems and propositions
\usepackage{amsthm}

% making logically defined graphics
%\usepackage{xypic}
% used for TeXing text within eps files
%\usepackage{psfrag}

% there are many more packages, add them here as you need them

% define commands here

\DeclareMathOperator{\Hom}{Hom}

\newcommand{\vecify}{{\mathcal V}}
\newcommand{\Act}{{A}}
\newcommand{\act}{{a}}
\newcommand{\Sit}{{S}}
\newcommand{\occ}{{v}}
\newcommand{\univ}{{\mathbf D}}
\newcommand{\uout}{{d_{out}}}
\newcommand{\uin}{{d_{in}}}
\newcommand{\mangle}{{\mathbf C}}

\newcommand{\psheaf}{{\mathcal F}}
\newcommand{\scat}{{\mathtt{Stoch}}}
\newcommand{\subs}{{\mathtt{Sys}}}
\newcommand{\mcat}{{\mathtt{Meas}}}
\newcommand{\eop}{{$\blacksquare$}}
\newcommand{\eod}{{${}$\\}}
\newcommand{\bra}{{\langle}}
\newcommand{\ket}{{\rangle}}

\newcommand{\cN}{{\mathcal N}}
\newcommand{\bR}{{\mathbb R}}
\newcommand{\fm}{{\mathfrak m}}
\newcommand{\cP}{{\mathcal P}}

\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{remark}
\newtheorem{eg}{Example}
\newtheorem{rem}{Remark}
\newtheorem{defn}{Definition}

\begin{document}
Any classical physical system (by which we simply mean any deterministic 
function) can be taken as a measuring apparatus or input/output device. 
For example, a thermometer takes inputs from the atmosphere and outputs 
numbers on a digital display. The thermometer categorizes inputs by 
temperature and is blind to, say, differences in air pressure. 

Classical measurements are formalized as follows:
\begin{defn}
    \label{d:cmeasure}
	Given a classical physical system with state space $X$, a 
    \emph{measuring device} is a function $f:X\rightarrow \bR$. 
    The output $r \in \bR$ is the \emph{reading} and the pre-image 
    $f^{-1}(r)\subset X$ is the \emph{measurement}.
\end{defn}

From this point of view a thermometer and a barometer are two functions, 
$T: X\rightarrow \bR$ and $B: X\rightarrow \bR$, mapping the state space 
$X$ of configurations (positions and momenta) of atmospheric particles 
to real numbers. When the thermometer outputs $2^\circ$, it specifies that 
the atmospheric configuration was in the pre-image $T^{-1}(2^\circ)$ which, 
assuming the thermometer perfectly measures temperature, is \emph{exactly}
characterized as atmospheric configurations with temperature $2^\circ$. 
Similarly, the pre-images generated by the barometer group atmospheric 
configurations by pressure.

The classical definition of measurement takes a thermometer as a monolithic 
object described by a single function from atmospheric configurations to 
real numbers. The internal structure of the thermometer -- that is composed
of countless atoms and molecules arranged in an extremely specific manner 
-- is swept under the carpet (or, rather, into the function).

This paper investigates the structure of measurements performed by 
\emph{distributed} systems. We do so by adapting Definition~\ref{d:cmeasure}
to a large class of systems that contains networks of Boolean functions 
\cite{Kauffman:2003fv}, Conway's game of life \cite{gardner:70, berlekamp:82}
and Hopfield networks \cite{hopfield:82, amit:89} as special cases.

Our motivation comes from prior work investigating information processing 
in discrete neural networks \cite{bt:08, bt:09}. The brain $X$ can be 
thought of as an enormously complicated measuring device $S\times 
X\xrightarrow{f} X$ mapping sensory states $s\in S$ and prior brain states 
$x\in X$ to subsequent brain states. Analyzing the functional dependencies 
implicit in cortical computations reduces to analyzing how the measurements 
performed by the brain are composed out of submeasurements by subdevices 
such as individual neurons and neuronal assemblies. The cortex is of 
particular interest since it seemingly effortlessly integrates diverse 
contextual data into a unified gestalt that determines behavior. The 
measurements performed by different neurons appear to interact in such 
a way that they generate more information jointly than separately. To 
improve our understanding of how the cortex integrates information we need 
to a formal language for analyzing how context affects measurements in 
distributed systems. 

As a first step in this direction, we develop methods for analyzing the 
geometry of measurements performed by functions with overlapping domains. 
We propose, roughly speaking, to study context-dependence in terms of the 
geometry of intersecting pre-images. However, since we wish to work with 
both probabilistic and deterministic systems, things are a bit more 
complicated. 

We sketch the contents of the paper. Section %\S\ref{s:stochastic} 
\PMlinkname{\S 2}{2stochasticmaps}
lays the groundwork by introducing the category of stochastic maps $\scat$. 
Our goal is to study finite set valued functions and conditional probability
distributions on finite sets. However, rather than work with sets, functions
and conditional distributions, we prefer to study stochastic maps (Markov 
matrices) between function spaces on sets. We therefore introduce the 
faithful functor $\vecify$ taking functions on sets to Markov matrices:
\begin{equation*}
	\Big[f:X\rightarrow Y\Big]\mapsto\Big[\vecify f:\vecify X\rightarrow 
    \vecify Y\Big],
\end{equation*}
where $\vecify X$ is functions from $X$ to $\bR$. Conditional probability 
distributions $p(y|x)$ can also be represented using stochastic maps. 

Working with linear operators instead of set-valued functions is convenient 
for two reasons. First, it unifies the deterministic and probabilistic cases
in a single language. Second, the dual $T^\natural$ of a stochastic map $T$ 
provides a symmetric treatment of functions and their corresponding inverse 
image functions. Recall the inverse of function $f:X\rightarrow Y$ is 
$f^{-1}:Y\rightarrow \underline{2}^X$, which takes values in the 
\emph{powerset} of $X$, rather than $X$ itself. Dualizing a stochastic map 
flips the domain and range of the original map, without introducing any new 
objects:
\begin{equation}
	\Big[f^{-1}:Y\rightarrow \underline{2}^X\Big]\,\mbox{ corresponds to }\,
	\Big[(\vecify f)^\natural:\vecify Y\rightarrow \vecify X\Big],
	\label{e:preimage-corr}
\end{equation}
see %Proposition~\ref{t:preimage}.
\PMlinkname{Corollary 2}{2stochasticmaps#Thmthm2}

Section %\S\ref{s:fds} 
\PMlinkname{\S 3}{3distributeddynamicalsystems}
introduces distributed dynamical systems. These extend 
probabilistic cellular automata by replacing cells (space 
coordinates) with occasions (spacetime coordinates: cell $k$ 
at time $t$). Inspired by \cite{hooft:99, abramsky:09},  we 
treat distributed systems as collections of stochastic maps 
between function spaces so that processes (stochastic maps) 
take center stage, rather than their outputs. %The resulting 
framework bares a formal resemblance to the categorical 
approach to quantum mechanics developed in \cite{abramsky:09}. 
Although the setting is abstract, it has the advantage that it 
is \emph{scalable}: using a coarse-graining procedure 
introduced in \cite{balduzzi:11} we can analyze distributed 
systems at any spatiotemporal granularity.

Distributed dynamical systems provide a rich class of toy 
universes. However, since these toy universes do not contain 
conscious observers we confront Bell's problem  \cite{bell:90}:
``What exactly qualifies some physical [system] to play the 
role of `measurer'?'' In our setting, where we do not have to 
worry about collapsing wave-functions or the distinction 
between macroscopic and microscopic processes, the solution is 
simple: \emph{every} physical system plays the role of 
measurer. More precisely, we track measurers via the 
category $\subs_\univ$ of subsystems of $\univ$. Each 
subsystem $\mangle$ is equipped with a mechanism $\fm_\mangle$ 
which is constructed by gluing together the mechanisms of the 
occasions in $\mangle$ and averaging over extrinsic noise.

Measuring devices are typically analyzed by varying their 
inputs and observing the effect on their outputs. By contrast 
this paper fixes the output and \emph{varies the device over 
all its subdevices} to obtain a family of submeasurements 
parametrized by all subsystems in $\subs_\univ$. The internal 
structure of the measurement performed by $\univ$ is then 
studied by comparing submeasurements.

We keep track of submeasurements by observing that they are 
sections of a suitably defined presheaf. Sheaf theory provides 
a powerful machinery for analyzing relationships between 
objects and subobjects \cite{maclane:92}, which we adapt to 
our setting by introducing the structure presheaf $\psheaf$, 
a contravariant functor from  $\subs_\univ$ to the category 
of measuring devices $\mcat_\univ$ on $\univ$. Importantly, 
$\psheaf$ is \emph{not} a sheaf: although the gluing axiom 
holds, uniqueness fails, see %Theorem~\ref{t:presheaf}
\PMlinkname{Theorem 4}{3distributeddynamicalsystems#Thmthm4}. 
This is because the restriction operator in $\mcat$ is 
(essentially) marginalization, and of course there are 
infinitely many joint distributions $p(x,y)$ that yield 
marginals $p(x)$ and $p(y)$. 

Section %\S\ref{s:measurement}
\PMlinkname{\S 4}{4measurement}
adapts Definition~\ref{d:cmeasure} to distributed systems and 
introduces the simplest quantity associated with a measurement:
effective information, which quantifies its precision, see 
%Proposition \ref{t:classmeas}
\PMlinkname{Proposition 5}{4measurement#Thmthm5}. 
Crucially, effective information is \emph{context-dependent} --
it is computed relative to a baseline which may be completely 
uninformative (the so-called null system) or provided by a 
subsystem. 

Finally entanglement, introduced in %\S\ref{s:tangle}
\PMlinkname{\S 5}{5entanglement}, 
quantifies the obstruction (in bits) to decomposing a 
measurement into independent submeasurements. It turns out, 
see discussion after %Theorem~\ref{t:g_ei}
\PMlinkname{Theorem 10}{5entanglement#Thmthm10}, 
that entanglement quantifies the extent to which a measurement 
is context-dependent -- the extent to which contextual 
information provided by one submeasurement is useful in 
understanding another. %Theorem~\ref{t:gamma} 
\PMlinkname{Theorem 9}{5entanglement#Thmthm9}
shows that a measurement is more precise than the sum of its 
submeasurements \emph{only if} entanglement is non-zero. 
Precision is thus inextricably bound to context-dependence 
and indecomposability. The failure of unique descent is thus 
a feature, not a bug, since it provides ``elbow room'' to 
build measuring devices that are \emph{not} products of 
subdevices. 

Space constraints prevent us from providing concrete examples; 
the interested reader can find these in \cite{bt:08, bt:09, 
balduzzi:11}. Our running examples are the deterministic 
set-valued functions
\begin{equation*}
	f:X\rightarrow Y\,\,\,\mbox{ and }\,\,\,g:X\times Y
    \rightarrow Z
\end{equation*}
which we use to illustrate the concepts as they are developed. 

\begin{thebibliography}{10}
%\providecommand{\bibitemdeclare}[2]{}
\providecommand{\urlprefix}{Available at }
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{\texttt{#2}}
\providecommand{\urlalt}[2]{\href{#1}{#2}}
\providecommand{\doi}[1]{doi:\urlalt{http://dx.doi.org/#1}{#1}}
\providecommand{\bibinfo}[2]{#2}

\bibitem{abramsky:09}
\bibinfo{author}{Samson Abramsky} \& \bibinfo{author}{Bob Coecke}
  (\bibinfo{year}{2009}): \emph{\bibinfo{title}{Categorical {Q}uantum
  {M}echanics}}.
\newblock In \bibinfo{editor}{K~Engesser}, \bibinfo{editor}{D~M Gabbay} \&
  \bibinfo{editor}{D~Lehmann}, editors: {\sl \bibinfo{booktitle}{Handbook of
  {Q}uantum {L}ogic and {Q}uantum {S}tructures: {Q}uantum {L}ogic}},
  \bibinfo{publisher}{Elsevier}.

%\bibitemdeclare{book}{amit:89}
\bibitem{amit:89}
\bibinfo{author}{DJ~Amit} (\bibinfo{year}{1989}):
  \emph{\bibinfo{title}{Modelling brain function: the world of attractor neural
  networks}}.
\newblock \bibinfo{publisher}{Cambridge University Press}.

%\bibitemdeclare{article}{balduzzi:11}
\bibitem{balduzzi:11}
\bibinfo{author}{David Balduzzi} (\bibinfo{year}{2011}):
  \emph{\bibinfo{title}{Detecting emergent processes in cellular automata with
  excess information}}.
\newblock {\sl \bibinfo{journal}{preprint}} .

%\bibitemdeclare{article}{bt:08}
\bibitem{bt:08}
\bibinfo{author}{David Balduzzi} \& \bibinfo{author}{Giulio Tononi}
  (\bibinfo{year}{2008}): \emph{\bibinfo{title}{Integrated {I}nformation in
  {D}iscrete {D}ynamical {S}ystems: {M}otivation and {T}heoretical
  {F}ramework.}}
\newblock {\sl \bibinfo{journal}{PLoS Comput Biol}}
  \bibinfo{volume}{4}(\bibinfo{number}{6}), p. \bibinfo{pages}{e1000091},
  \doi{10.1371/journal.pcbi.1000091}.

%\bibitemdeclare{article}{bt:09}
\bibitem{bt:09}
\bibinfo{author}{David Balduzzi} \& \bibinfo{author}{Giulio Tononi}
  (\bibinfo{year}{2009}): \emph{\bibinfo{title}{Qualia: the geometry of
  integrated information}}.
\newblock {\sl \bibinfo{journal}{PLoS Comput Biol}}
  \bibinfo{volume}{5}(\bibinfo{number}{8}), p. \bibinfo{pages}{e1000462},
  \doi{10.1371/journal.pcbi.1000462}.

%\bibitemdeclare{article}{bell:90}
\bibitem{bell:90}
\bibinfo{author}{J~S Bell} (\bibinfo{year}{1990}):
  \emph{\bibinfo{title}{Against `{M}easurement'}}.
\newblock {\sl \bibinfo{journal}{Physics World}} \bibinfo{volume}{August}, pp.
  \bibinfo{pages}{33--40}.

%\bibitemdeclare{article}{gardner:70}
\bibitem{gardner:70}
\bibinfo{author}{Martin Gardner} (\bibinfo{year}{1970}):
  \emph{\bibinfo{title}{Mathematical {G}ames - {T}he {F}antastic {C}ombinations
  of {J}ohn {C}onway's {N}ew {S}olitaire {G}ame, {L}ife}}.
\newblock {\sl \bibinfo{journal}{Scientific American}} \bibinfo{volume}{223},
  pp. \bibinfo{pages}{120--123}.
 
%\bibitemdeclare{article}{hooft:99}
\bibitem{hooft:99}
\bibinfo{author}{G~'t~Hooft} (\bibinfo{year}{1999}):
  \emph{\bibinfo{title}{Quantum gravity as a dissipative deterministic
  system}}.
\newblock {\sl \bibinfo{journal}{Classical and Quantum Gravity}}
  \bibinfo{volume}{16}(\bibinfo{number}{10}).

%\bibitemdeclare{article}{hopfield:82}
\bibitem{hopfield:82}
\bibinfo{author}{JJ~Hopfield} (\bibinfo{year}{1982}):
  \emph{\bibinfo{title}{Neural networks and physical systems with emergent
  computational properties}}.
\newblock {\sl \bibinfo{journal}{Proc. Nat. Acad. Sci.}} \bibinfo{volume}{79},
  pp. \bibinfo{pages}{2554--2558}.

\bibitem{Kauffman:2003fv}
\bibinfo{author}{Stuart Kauffman}, \bibinfo{author}{Carsten Peterson},
  \bibinfo{author}{Bj{\"o}rn Samuelsson} \& \bibinfo{author}{Carl Troein}
  (\bibinfo{year}{2003}): \emph{\bibinfo{title}{Random Boolean network models
  and the yeast transcriptional network}}.
\newblock {\sl \bibinfo{journal}{Proc Natl Acad Sci U S A}}
  \bibinfo{volume}{100}(\bibinfo{number}{25}), pp. \bibinfo{pages}{14796--9},
  \doi{10.1073/pnas.2036429100}.

%\bibitemdeclare{book}{maclane:92}
\bibitem{maclane:92}
\bibinfo{author}{S~MacLane} \& \bibinfo{author}{Ieke Moerdijk}
  (\bibinfo{year}{1992}): \emph{\bibinfo{title}{Sheaves in {G}eometry and
  {L}ogic: {A} {F}irst {I}ntroduction to {T}opos {T}heory}}.
\newblock \bibinfo{publisher}{Springer}.

\end{thebibliography}
\end{document}
